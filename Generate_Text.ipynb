{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 831,
     "status": "ok",
     "timestamp": 1563831801795,
     "user": {
      "displayName": "Shivangi Gupta",
      "photoUrl": "",
      "userId": "12757452847987614308"
     },
     "user_tz": 420
    },
    "id": "rzz8f9NJrzyV",
    "outputId": "a6a6ee84-d368-445e-d4c6-51fe6e6de1c0"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f11508e3d107>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcifar10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\software\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\software\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgeneric_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\software\\lib\\site-packages\\keras\\utils\\io_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mHDF5_OBJECT_HEADER_LIMIT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64512\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\software\\lib\\site-packages\\h5py\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0m_errors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msilence_errors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_conv\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mregister_converters\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_register_converters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[0m_register_converters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\h5r.pxd\u001b[0m in \u001b[0;36minit h5py._conv\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pxd\u001b[0m in \u001b[0;36minit h5py.h5r\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mF:\\SOFTWARE\\Lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36mparent\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import cv2\n",
    "from keras import backend as K\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten,MaxPooling2D,Embedding,SpatialDropout1D,LSTM\n",
    "from keras.utils import to_categorical\n",
    "import copy\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import re\n",
    "from google.colab import drive\n",
    "from sklearn.preprocessing import LabelBinarizer \n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 832,
     "status": "ok",
     "timestamp": 1563832094432,
     "user": {
      "displayName": "Shivangi Gupta",
      "photoUrl": "",
      "userId": "12757452847987614308"
     },
     "user_tz": 420
    },
    "id": "8qsdnd9HIILH",
    "outputId": "c94133aa-e28c-4433-fe87-414545733f97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from reading the books: \n",
      "OUR KNOWLEDGE OF THE  EXTERNAL WORLD  AS A FIELD FOR SCIENTIFIC METHOD  IN PHILOSOPHYBY THE SAME AUTHOR  INTRODUCTION TO MATHEMATICAL PHILOSOPHY  Second Edition.  Demy 8vo, 12s. 6d. net.  THE ANALYSIS OF MIND  Demy 8vo, 16s. net.  PRINCIPLES OF SOCIAL RECONSTRUCTION  Seventh Impression.  Cr. 8vo, 5s. net; Limp, 3s. 6d. net.  ROADS TO FREEDOM: SOCIALISM, ANARCHISM AND SYNDICALISM  Fourth Impression.  Cr. 8vo, 5s. net; Limp, 3s. 6d. net.  THE PRACTICE AND THEORY OF BOLSHEVISM  Second Impression.  Cr. 8vo, 6s. net.  OUR KNOWLEDGE OF  THE EXTERNAL WORLD  AS A FIELD FOR SCIENTIFIC METHOD  IN PHILOSOPHY  BY  BERTRAND RUSSELL, F.R.S  LONDON: GEORGE ALLEN & UNWIN LTD  RUSKIN HOUSE, 40 MUSEUM STREET, W.C. 1  First published in 1914 by  The Open Court Publishing Company  Reissued by George Allen & Unwin Ltd.  1922PREFACEThe following lectures[1] are an attempt to show, by means of examples,the nature, capacity, and limitations of the logical-analytic method inphilosophy. This method, of which th\n"
     ]
    }
   ],
   "source": [
    "#1c(i)\n",
    "f1=open(\"/content/drive/My Drive/BertrandRusell.txt\", 'rb')\n",
    "Br_text=(f1.read().decode().replace(\"\\n\",\"\").replace(\"\\r\",\"\").replace(\"\\t\",\"\"))\n",
    "print(\"Text from reading the books: \")\n",
    "print(Br_text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 925,
     "status": "ok",
     "timestamp": 1563832099173,
     "user": {
      "displayName": "Shivangi Gupta",
      "photoUrl": "",
      "userId": "12757452847987614308"
     },
     "user_tz": 420
    },
    "id": "ggMQlKRCjOOk",
    "outputId": "643b1894-09e7-43ea-91a6-6a6e8f3de1c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'h': 0.8, ';': 0.3, '4': 0.222, 'n': 0.867, 'l': 0.844, 'o': 0.878, 'x': 0.978, 's': 0.922, 'b': 0.733, '8': 0.267, '9': 0.278, '3': 0.211, 'u': 0.944, '.': 0.156, 'g': 0.789, 'c': 0.744, '2': 0.2, '6': 0.244, 'j': 0.822, ',': 0.133, 'e': 0.767, '+': 0.122, 'f': 0.778, ' ': 0.0, 'r': 0.911, 'd': 0.756, 'v': 0.956, '1': 0.189, '5': 0.233, 'y': 0.989, 'm': 0.856, '_': 0.7, 'p': 0.889, 't': 0.933, 'z': 1.0, 'k': 0.833, 'i': 0.811, '7': 0.256, 'w': 0.967, '0': 0.178, 'q': 0.9, 'a': 0.722}\n"
     ]
    }
   ],
   "source": [
    "#1c(ii)\n",
    "Br_text=Br_text.lower()\n",
    "Br_text=re.sub('[^a-zA-Z0-9.,_+;\\s]+', '', Br_text)\n",
    "dictionary = dict()\n",
    "max1=0\n",
    "min1=1000\n",
    "for ch in list(set(Br_text)):\n",
    "    dictionary[ch] = ord(ch)\n",
    "    if(ord(ch)>max1):\n",
    "        max1=ord(ch)\n",
    "    if(ord(ch)<min1):\n",
    "        min1=ord(ch)\n",
    "\n",
    "for k,v in dictionary.items():\n",
    "    dictionary[k]=round((v-min1)/(max1-min1),3)\n",
    "    \n",
    "print(dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 817,
     "status": "ok",
     "timestamp": 1563832103961,
     "user": {
      "displayName": "Shivangi Gupta",
      "photoUrl": "",
      "userId": "12757452847987614308"
     },
     "user_tz": 420
    },
    "id": "4J96y8eROajS",
    "outputId": "c2c97487-ff95-4ff6-bc72-d7057f486faa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window size : 100\n"
     ]
    }
   ],
   "source": [
    "#1c(iii)\n",
    "print(\"Window size : 100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46373,
     "status": "ok",
     "timestamp": 1563832155563,
     "user": {
      "displayName": "Shivangi Gupta",
      "photoUrl": "",
      "userId": "12757452847987614308"
     },
     "user_tz": 420
    },
    "id": "-Bvs5euhjYZg",
    "outputId": "936af6ea-b454-4b00-ddbb-ad970032cc7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input train data fed into the network:\n",
      "[[0.878, 0.944, 0.911, 0.0, 0.833, 0.867, 0.878, 0.967, 0.844, 0.767, 0.756, 0.789, 0.767, 0.0, 0.878, 0.778, 0.0, 0.933, 0.8, 0.767, 0.0, 0.0, 0.767, 0.978, 0.933, 0.767, 0.911, 0.867, 0.722, 0.844, 0.0, 0.967, 0.878, 0.911, 0.844, 0.756, 0.0, 0.0, 0.722, 0.922, 0.0, 0.722, 0.0, 0.778, 0.811, 0.767, 0.844, 0.756, 0.0, 0.778, 0.878, 0.911, 0.0, 0.922, 0.744, 0.811, 0.767, 0.867, 0.933, 0.811, 0.778, 0.811, 0.744, 0.0, 0.856, 0.767, 0.933, 0.8, 0.878, 0.756, 0.0, 0.0, 0.811, 0.867, 0.0, 0.889, 0.8, 0.811, 0.844, 0.878, 0.922, 0.878, 0.889, 0.8, 0.989, 0.733, 0.989, 0.0, 0.933, 0.8, 0.767, 0.0, 0.922, 0.722, 0.856, 0.767, 0.0, 0.722, 0.944], [0.944, 0.911, 0.0, 0.833, 0.867, 0.878, 0.967, 0.844, 0.767, 0.756, 0.789, 0.767, 0.0, 0.878, 0.778, 0.0, 0.933, 0.8, 0.767, 0.0, 0.0, 0.767, 0.978, 0.933, 0.767, 0.911, 0.867, 0.722, 0.844, 0.0, 0.967, 0.878, 0.911, 0.844, 0.756, 0.0, 0.0, 0.722, 0.922, 0.0, 0.722, 0.0, 0.778, 0.811, 0.767, 0.844, 0.756, 0.0, 0.778, 0.878, 0.911, 0.0, 0.922, 0.744, 0.811, 0.767, 0.867, 0.933, 0.811, 0.778, 0.811, 0.744, 0.0, 0.856, 0.767, 0.933, 0.8, 0.878, 0.756, 0.0, 0.0, 0.811, 0.867, 0.0, 0.889, 0.8, 0.811, 0.844, 0.878, 0.922, 0.878, 0.889, 0.8, 0.989, 0.733, 0.989, 0.0, 0.933, 0.8, 0.767, 0.0, 0.922, 0.722, 0.856, 0.767, 0.0, 0.722, 0.944, 0.933], [0.911, 0.0, 0.833, 0.867, 0.878, 0.967, 0.844, 0.767, 0.756, 0.789, 0.767, 0.0, 0.878, 0.778, 0.0, 0.933, 0.8, 0.767, 0.0, 0.0, 0.767, 0.978, 0.933, 0.767, 0.911, 0.867, 0.722, 0.844, 0.0, 0.967, 0.878, 0.911, 0.844, 0.756, 0.0, 0.0, 0.722, 0.922, 0.0, 0.722, 0.0, 0.778, 0.811, 0.767, 0.844, 0.756, 0.0, 0.778, 0.878, 0.911, 0.0, 0.922, 0.744, 0.811, 0.767, 0.867, 0.933, 0.811, 0.778, 0.811, 0.744, 0.0, 0.856, 0.767, 0.933, 0.8, 0.878, 0.756, 0.0, 0.0, 0.811, 0.867, 0.0, 0.889, 0.8, 0.811, 0.844, 0.878, 0.922, 0.878, 0.889, 0.8, 0.989, 0.733, 0.989, 0.0, 0.933, 0.8, 0.767, 0.0, 0.922, 0.722, 0.856, 0.767, 0.0, 0.722, 0.944, 0.933, 0.8], [0.0, 0.833, 0.867, 0.878, 0.967, 0.844, 0.767, 0.756, 0.789, 0.767, 0.0, 0.878, 0.778, 0.0, 0.933, 0.8, 0.767, 0.0, 0.0, 0.767, 0.978, 0.933, 0.767, 0.911, 0.867, 0.722, 0.844, 0.0, 0.967, 0.878, 0.911, 0.844, 0.756, 0.0, 0.0, 0.722, 0.922, 0.0, 0.722, 0.0, 0.778, 0.811, 0.767, 0.844, 0.756, 0.0, 0.778, 0.878, 0.911, 0.0, 0.922, 0.744, 0.811, 0.767, 0.867, 0.933, 0.811, 0.778, 0.811, 0.744, 0.0, 0.856, 0.767, 0.933, 0.8, 0.878, 0.756, 0.0, 0.0, 0.811, 0.867, 0.0, 0.889, 0.8, 0.811, 0.844, 0.878, 0.922, 0.878, 0.889, 0.8, 0.989, 0.733, 0.989, 0.0, 0.933, 0.8, 0.767, 0.0, 0.922, 0.722, 0.856, 0.767, 0.0, 0.722, 0.944, 0.933, 0.8, 0.878], [0.833, 0.867, 0.878, 0.967, 0.844, 0.767, 0.756, 0.789, 0.767, 0.0, 0.878, 0.778, 0.0, 0.933, 0.8, 0.767, 0.0, 0.0, 0.767, 0.978, 0.933, 0.767, 0.911, 0.867, 0.722, 0.844, 0.0, 0.967, 0.878, 0.911, 0.844, 0.756, 0.0, 0.0, 0.722, 0.922, 0.0, 0.722, 0.0, 0.778, 0.811, 0.767, 0.844, 0.756, 0.0, 0.778, 0.878, 0.911, 0.0, 0.922, 0.744, 0.811, 0.767, 0.867, 0.933, 0.811, 0.778, 0.811, 0.744, 0.0, 0.856, 0.767, 0.933, 0.8, 0.878, 0.756, 0.0, 0.0, 0.811, 0.867, 0.0, 0.889, 0.8, 0.811, 0.844, 0.878, 0.922, 0.878, 0.889, 0.8, 0.989, 0.733, 0.989, 0.0, 0.933, 0.8, 0.767, 0.0, 0.922, 0.722, 0.856, 0.767, 0.0, 0.722, 0.944, 0.933, 0.8, 0.878, 0.911]]\n",
      " \n",
      "Input train labels:\n",
      "[116, 104, 111, 114, 32]\n"
     ]
    }
   ],
   "source": [
    "#1c(iv)\n",
    "X_train=[]\n",
    "y_train=[]\n",
    "y_train_temp=[]\n",
    "for i in range(0,len(Br_text)):\n",
    "    if((i+99)>len(Br_text)-1):\n",
    "        break\n",
    "    temp=list(Br_text[i:i+99])\n",
    "    res=[]\n",
    "    for k in temp:\n",
    "        res.append(dictionary[k])\n",
    "    X_train.append(res) \n",
    "    y_train.append(dictionary[Br_text[i+99]])\n",
    "    y_train_temp.append(ord(Br_text[i+99]))\n",
    "    i=i+1\n",
    "print(\"Input train data fed into the network:\")\n",
    "print(X_train[0:5])\n",
    "print(\" \")\n",
    "print(\"Input train labels:\")\n",
    "print(y_train_temp[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2719,
     "status": "ok",
     "timestamp": 1563832179247,
     "user": {
      "displayName": "Shivangi Gupta",
      "photoUrl": "",
      "userId": "12757452847987614308"
     },
     "user_tz": 420
    },
    "id": "DdBzGrkvsjAp",
    "outputId": "ccc2b6b2-198a-4c6c-b54d-1d0463c8d28a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One hot encoded labels: \n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      "  0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      "  0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#1c(v)\n",
    "y_train1 = to_categorical(y_train_temp)\n",
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(y_train_temp)\n",
    "label_classes=label_binarizer.classes_\n",
    "y_train1=label_binarizer.transform(y_train_temp)\n",
    "print(\"One hot encoded labels: \")\n",
    "print(y_train1[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11937858,
     "status": "ok",
     "timestamp": 1563862922173,
     "user": {
      "displayName": "Shivangi Gupta",
      "photoUrl": "",
      "userId": "12757452847987614308"
     },
     "user_tz": 420
    },
    "id": "D_IdCAiGnCP0",
    "outputId": "bbd6337d-6e6e-45a7-8b7c-5467e34c796e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1739871/1739871 [==============================] - 492s 283us/step - loss: 2.8466 - acc: 0.1846\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.84664, saving model to weights_new.best.hdf5\n",
      "Epoch 2/30\n",
      "1739871/1739871 [==============================] - 493s 283us/step - loss: 2.7547 - acc: 0.2059\n",
      "\n",
      "Epoch 00002: loss improved from 2.84664 to 2.75468, saving model to weights_new.best.hdf5\n",
      "Epoch 3/30\n",
      "1739871/1739871 [==============================] - 492s 283us/step - loss: 2.6981 - acc: 0.2168\n",
      "\n",
      "Epoch 00003: loss improved from 2.75468 to 2.69814, saving model to weights_new.best.hdf5\n",
      "Epoch 4/30\n",
      "1739871/1739871 [==============================] - 492s 283us/step - loss: 2.6469 - acc: 0.2311\n",
      "\n",
      "Epoch 00004: loss improved from 2.69814 to 2.64692, saving model to weights_new.best.hdf5\n",
      "Epoch 5/30\n",
      "1739871/1739871 [==============================] - 493s 283us/step - loss: 2.6022 - acc: 0.2452\n",
      "\n",
      "Epoch 00005: loss improved from 2.64692 to 2.60222, saving model to weights_new.best.hdf5\n",
      "Epoch 6/30\n",
      "1739871/1739871 [==============================] - 493s 283us/step - loss: 2.5555 - acc: 0.2590\n",
      "\n",
      "Epoch 00006: loss improved from 2.60222 to 2.55548, saving model to weights_new.best.hdf5\n",
      "Epoch 7/30\n",
      "1739871/1739871 [==============================] - 493s 283us/step - loss: 2.5103 - acc: 0.2720\n",
      "\n",
      "Epoch 00007: loss improved from 2.55548 to 2.51033, saving model to weights_new.best.hdf5\n",
      "Epoch 8/30\n",
      "1739871/1739871 [==============================] - 493s 283us/step - loss: 2.4696 - acc: 0.2840\n",
      "\n",
      "Epoch 00008: loss improved from 2.51033 to 2.46959, saving model to weights_new.best.hdf5\n",
      "Epoch 9/30\n",
      "1739871/1739871 [==============================] - 493s 283us/step - loss: 2.4313 - acc: 0.2958\n",
      "\n",
      "Epoch 00009: loss improved from 2.46959 to 2.43125, saving model to weights_new.best.hdf5\n",
      "Epoch 10/30\n",
      "1739871/1739871 [==============================] - 493s 283us/step - loss: 2.3948 - acc: 0.3086\n",
      "\n",
      "Epoch 00010: loss improved from 2.43125 to 2.39480, saving model to weights_new.best.hdf5\n",
      "Epoch 11/30\n",
      "1739871/1739871 [==============================] - 493s 283us/step - loss: 2.3591 - acc: 0.3195\n",
      "\n",
      "Epoch 00011: loss improved from 2.39480 to 2.35910, saving model to weights_new.best.hdf5\n",
      "Epoch 12/30\n",
      "1739871/1739871 [==============================] - 493s 283us/step - loss: 2.3255 - acc: 0.3303\n",
      "\n",
      "Epoch 00012: loss improved from 2.35910 to 2.32552, saving model to weights_new.best.hdf5\n",
      "Epoch 13/30\n",
      "1739871/1739871 [==============================] - 494s 284us/step - loss: 2.2948 - acc: 0.3399\n",
      "\n",
      "Epoch 00013: loss improved from 2.32552 to 2.29483, saving model to weights_new.best.hdf5\n",
      "Epoch 14/30\n",
      "1739871/1739871 [==============================] - 494s 284us/step - loss: 2.2677 - acc: 0.3483\n",
      "\n",
      "Epoch 00014: loss improved from 2.29483 to 2.26772, saving model to weights_new.best.hdf5\n",
      "Epoch 15/30\n",
      "1739871/1739871 [==============================] - 494s 284us/step - loss: 2.2440 - acc: 0.3564\n",
      "\n",
      "Epoch 00015: loss improved from 2.26772 to 2.24404, saving model to weights_new.best.hdf5\n",
      "Epoch 16/30\n",
      "1739871/1739871 [==============================] - 494s 284us/step - loss: 2.2202 - acc: 0.3634\n",
      "\n",
      "Epoch 00016: loss improved from 2.24404 to 2.22022, saving model to weights_new.best.hdf5\n",
      "Epoch 17/30\n",
      "1739871/1739871 [==============================] - 495s 284us/step - loss: 2.1995 - acc: 0.3701\n",
      "\n",
      "Epoch 00017: loss improved from 2.22022 to 2.19948, saving model to weights_new.best.hdf5\n",
      "Epoch 18/30\n",
      "1739871/1739871 [==============================] - 494s 284us/step - loss: 2.1806 - acc: 0.3759\n",
      "\n",
      "Epoch 00018: loss improved from 2.19948 to 2.18060, saving model to weights_new.best.hdf5\n",
      "Epoch 19/30\n",
      "1739871/1739871 [==============================] - 495s 284us/step - loss: 2.1626 - acc: 0.3817\n",
      "\n",
      "Epoch 00019: loss improved from 2.18060 to 2.16265, saving model to weights_new.best.hdf5\n",
      "Epoch 20/30\n",
      "1739871/1739871 [==============================] - 495s 284us/step - loss: 2.1467 - acc: 0.3866\n",
      "\n",
      "Epoch 00020: loss improved from 2.16265 to 2.14672, saving model to weights_new.best.hdf5\n",
      "Epoch 21/30\n",
      "1739871/1739871 [==============================] - 494s 284us/step - loss: 2.1316 - acc: 0.3913\n",
      "\n",
      "Epoch 00021: loss improved from 2.14672 to 2.13161, saving model to weights_new.best.hdf5\n",
      "Epoch 22/30\n",
      "1739871/1739871 [==============================] - 495s 284us/step - loss: 2.1169 - acc: 0.3960\n",
      "\n",
      "Epoch 00022: loss improved from 2.13161 to 2.11692, saving model to weights_new.best.hdf5\n",
      "Epoch 23/30\n",
      "1739871/1739871 [==============================] - 495s 284us/step - loss: 2.1085 - acc: 0.3988\n",
      "\n",
      "Epoch 00023: loss improved from 2.11692 to 2.10849, saving model to weights_new.best.hdf5\n",
      "Epoch 24/30\n",
      "1739871/1739871 [==============================] - 495s 284us/step - loss: 2.0928 - acc: 0.4036\n",
      "\n",
      "Epoch 00024: loss improved from 2.10849 to 2.09277, saving model to weights_new.best.hdf5\n",
      "Epoch 25/30\n",
      "1739871/1739871 [==============================] - 494s 284us/step - loss: 2.0813 - acc: 0.4066\n",
      "\n",
      "Epoch 00025: loss improved from 2.09277 to 2.08128, saving model to weights_new.best.hdf5\n",
      "Epoch 26/30\n",
      "1739871/1739871 [==============================] - 494s 284us/step - loss: 2.0673 - acc: 0.4107\n",
      "\n",
      "Epoch 00026: loss improved from 2.08128 to 2.06729, saving model to weights_new.best.hdf5\n",
      "Epoch 27/30\n",
      "1739871/1739871 [==============================] - 493s 283us/step - loss: 2.0568 - acc: 0.4139\n",
      "\n",
      "Epoch 00027: loss improved from 2.06729 to 2.05683, saving model to weights_new.best.hdf5\n",
      "Epoch 28/30\n",
      "1739871/1739871 [==============================] - 493s 283us/step - loss: 2.0462 - acc: 0.4174\n",
      "\n",
      "Epoch 00028: loss improved from 2.05683 to 2.04623, saving model to weights_new.best.hdf5\n",
      "Epoch 29/30\n",
      "1739871/1739871 [==============================] - 492s 283us/step - loss: 2.0371 - acc: 0.4201\n",
      "\n",
      "Epoch 00029: loss improved from 2.04623 to 2.03708, saving model to weights_new.best.hdf5\n",
      "Epoch 30/30\n",
      "1739871/1739871 [==============================] - 491s 282us/step - loss: 2.0271 - acc: 0.4229\n",
      "\n",
      "Epoch 00030: loss improved from 2.03708 to 2.02712, saving model to weights_new.best.hdf5\n"
     ]
    }
   ],
   "source": [
    "#1c(vi,vii,viii)\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(256,input_shape=( 99, 1)))\n",
    "model_lstm.add(Dense(42, activation = 'softmax'))\n",
    "model_lstm.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='Adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "filepath=\"weights_new.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "#callbacks_list = [checkpoint]\n",
    "model_lstm.save_weights('/content/drive/My Drive/Colab Notebooks/HW7Weights1/BertrandRusell/'+filepath)\n",
    "history = model_lstm.fit(\n",
    "    np.array(X_train).reshape(len(X_train),99,1),\n",
    "    np.array(y_train1),\n",
    "    epochs = 30,\n",
    "    batch_size=2000,\n",
    "    callbacks=[checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 842,
     "status": "ok",
     "timestamp": 1563846934766,
     "user": {
      "displayName": "Shivangi Gupta",
      "photoUrl": "",
      "userId": "12757452847987614308"
     },
     "user_tz": 420
    },
    "id": "SKbLM46sWekm",
    "outputId": "b142e216-ea97-4316-9085-6a83af29cee4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0.\n"
     ]
    }
   ],
   "source": [
    "print(\"Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 880,
     "status": "ok",
     "timestamp": 1563828663661,
     "user": {
      "displayName": "Shivangi Gupta",
      "photoUrl": "",
      "userId": "12757452847987614308"
     },
     "user_tz": 420
    },
    "id": "wqT2hWK9QhEf",
    "outputId": "d9bdcd90-eca5-4708-9313-23ee1756d6e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs: 30\n"
     ]
    }
   ],
   "source": [
    "#1c(ix)\n",
    "print(\"Number of epochs: 30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5hktYg5c5PjJ"
   },
   "outputs": [],
   "source": [
    "#1c(x,xi)\n",
    "test_data=\"There are those who take mental phenomena naively, just as they \\\n",
    "would physical phenomena. This school of psychologists tends not to \\\n",
    "emphasize the object.\"\n",
    "test_data=test_data.lower()\n",
    "test_data=re.sub('[^a-zA-Z0-9.,_+;\\s]+', '', test_data)\n",
    "#print(test_data)\n",
    "\n",
    "\n",
    "model_lstm.load_weights('/content/drive/My Drive/Colab Notebooks/HW7Weights1/BertrandRusell/'+filepath)\n",
    "model_lstm.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='Adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "final_res=\"\"\n",
    "for l in range(0,1000):\n",
    "  X_test=[]\n",
    "  test_data=test_data[-99:]\n",
    "  #print(test_data)\n",
    "  for i in list(test_data):\n",
    "    X_test.append(dictionary[i]) \n",
    "  pred_test=list(model_lstm.predict(np.array(X_test).reshape(1,99,1))[0])\n",
    "  #print(len(pred_test))\n",
    "  r=label_classes[pred_test.index(max(pred_test))]\n",
    "\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "  final_res=final_res+str(reverse_dictionary[round((r-min1)/(max1-min1),3)])\n",
    "  test_data=test_data[-99:]+str(reverse_dictionary[round((r-min1)/(max1-min1),3)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 976,
     "status": "ok",
     "timestamp": 1563863246948,
     "user": {
      "displayName": "Shivangi Gupta",
      "photoUrl": "",
      "userId": "12757452847987614308"
     },
     "user_tz": 420
    },
    "id": "ontQe_9sg0-y",
    "outputId": "16cc1da4-db56-46ac-a40a-de8ce8fc9eff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated characters from test data:\n",
      " wh say that the whrle of a teinn as a pensieceite shetgy of the poenent soeersannty oo she ooenenpes of the sorec oeraeption. and the poentsoes oe the poenestee anteerse aednnnticcted by the pajt the pajtc ane the same theng is an antertarid oatervel than the sare ald the soaneter tfe seeentes that the pajtc ase not aeloe and ane ane the semees of a teinn and the soaneter of the poenent soeersannty. bnt if the case of a sense ane anlears the sare oane is an antertariog of the poenent soeersannty of the poenent soeersannty of the poenent soeersannty of the pajtc as a poenent and the same theng is an antertarid oftsie aydee than the same theng is an antertarid oatervel thenry of the poenent soeersannty oo she ooenenpes of the sorec oeraeption. and the poentsoes oe the poenestee anteerse aednnnticcted by the pajt the pajtc ane the same theng is an antertarid oatervel than the sare ald the soaneter tfe seeentes that the pajtc ase not aeloe and ane ane the semees of a teinn and the soanete\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated characters from test data:\")\n",
    "print(final_res)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW7.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
